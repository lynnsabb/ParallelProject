================================================================================
PARALLEL DATA ANALYTICS PROJECT - COMPLETE GUIDE
================================================================================

This document contains everything about the project: what was done, how to use it,
performance results, and analysis.

================================================================================
PART 1: PROJECT OVERVIEW
================================================================================

PROJECT: Parallel Data Analytics Across Architectures
STATUS: ✅ COMPLETE - All requirements met

Dataset: Australian Weather Dataset (weatherAUS.csv)
- 58,236 records (after filtering)
- 16 numerical features
- Source: Kaggle

Algorithm: Statistical Feature Extraction
- Correlation matrix computation (Pearson correlation)
- Statistical moments (mean, variance, skewness)
- Feature normalization
- Complexity: O(n × m²) - highly parallelizable

================================================================================
PART 2: IMPLEMENTATIONS CREATED
================================================================================

1. SEQUENTIAL (sequential.c)
   - Baseline single-threaded implementation
   - Time: 0.0910 seconds
   - Compile: gcc -O3 -std=c11 -o sequential.exe sequential.c -lm

2. PTHREADS (pthreads.c)
   - CPU-level thread parallelism
   - Configurable: 2, 4, 8 threads
   - Compile: gcc -O3 -std=c11 -o pthreads.exe pthreads.c -lm -lpthread
   - Run: .\pthreads.exe weatherAUS.csv <num_threads>

3. OPENMP (openmp.c)
   - Shared-memory parallelism
   - Schedules: static, dynamic, guided
   - Compile: gcc -O3 -std=c11 -fopenmp -o openmp.exe openmp.c -lm
   - Run: .\openmp.exe weatherAUS.csv <threads> <schedule>

4. MPI (mpi.c)
   - Distributed-memory parallelism
   - Configurable: 2, 4, 8 processes
   - Compile: Use x64 Native Tools Command Prompt
     cl /O2 /EHsc /I"C:\Program Files (x86)\Microsoft SDKs\MPI\Include" mpi.c /link /LIBPATH:"C:\Program Files (x86)\Microsoft SDKs\MPI\Lib\x64" msmpi.lib /out:mpi.exe
   - Run: mpiexec -n <processes> .\mpi.exe weatherAUS.csv

5. CUDA (cuda.cu)
   - GPU parallelism
   - Two kernels: simple, tiled
   - Compile: Use compile_cuda.bat or x64 Native Tools
     nvcc -O0 -o cuda.exe cuda.cu
   - Run: .\cuda.exe weatherAUS.csv <simple|tiled>

================================================================================
PART 3: PERFORMANCE RESULTS (RECORDED DATA)
================================================================================

TEST ENVIRONMENT:
- Windows 10
- Dataset: 58,236 records, 16 features
- Date: November 25, 2025

PERFORMANCE DATA:

Sequential (Baseline):
    Time: 0.0910 seconds
    Speedup: 1.00×

Pthreads:
    2 threads:  0.0540 seconds  |  Speedup: 1.69×
    4 threads:  0.0330 seconds  |  Speedup: 2.76×
    8 threads:  0.0210 seconds  |  Speedup: 4.33×

OpenMP:
    2 threads, static:  0.0650 seconds  |  Speedup: 1.40×
    4 threads, static:  0.0410 seconds  |  Speedup: 2.22×
    8 threads, static:  0.0330 seconds  |  Speedup: 2.76×
    8 threads, dynamic: 0.0260 seconds  |  Speedup: 3.50×
    8 threads, guided:  0.0240 seconds  |  Speedup: 3.79×

MPI:
    2 processes:  0.0387 seconds  |  Speedup: 2.35×
    4 processes:  0.0204 seconds  |  Speedup: 4.46×
    8 processes:  0.0126 seconds  |  Speedup: 7.22×

CUDA:
    Simple kernel:  0.0039 seconds  |  Speedup: 23.33×
    Tiled kernel:   0.0039 seconds  |  Speedup: 23.33×

================================================================================
PART 4: PERFORMANCE COMPARISON TABLE
================================================================================

Implementation          | Configuration        | Time (s) | Speedup
------------------------|----------------------|----------|---------
Sequential             | Baseline             | 0.0910   | 1.00×
Pthreads               | 2 threads            | 0.0540   | 1.69×
Pthreads               | 4 threads            | 0.0330   | 2.76×
Pthreads               | 8 threads            | 0.0210   | 4.33×
OpenMP                 | 2 threads, static    | 0.0650   | 1.40×
OpenMP                 | 4 threads, static     | 0.0410   | 2.22×
OpenMP                 | 8 threads, static     | 0.0330   | 2.76×
OpenMP                 | 8 threads, dynamic    | 0.0260   | 3.50×
OpenMP                 | 8 threads, guided     | 0.0240   | 3.79×
MPI                    | 2 processes          | 0.0387   | 2.35×
MPI                    | 4 processes         | 0.0204   | 4.46×
MPI                    | 8 processes         | 0.0126   | 7.22×
CUDA                   | Simple kernel        | 0.0039   | 23.33×
CUDA                   | Tiled kernel         | 0.0039   | 23.33×

================================================================================
PART 5: SPEEDUP CALCULATIONS
================================================================================

Formula: Speedup = Sequential Time / Parallel Time

Calculations:
- Pthreads 2:  0.0910 / 0.0540 = 1.69×
- Pthreads 4:  0.0910 / 0.0330 = 2.76×
- Pthreads 8:  0.0910 / 0.0210 = 4.33×
- OpenMP 8 guided: 0.0910 / 0.0240 = 3.79×
- MPI 2:  0.0910 / 0.0387 = 2.35×
- MPI 4:  0.0910 / 0.0204 = 4.46×
- MPI 8:  0.0910 / 0.0126 = 7.22×
- CUDA: 0.0910 / 0.0039 = 23.33×

================================================================================
PART 6: KEY FINDINGS & ANALYSIS
================================================================================

PERFORMANCE RANKING:
1. CUDA: 23.33× (GPU massive parallelism)
2. MPI 8 processes: 7.22× (Best CPU performance)
3. Pthreads 8 threads: 4.33× (Good shared memory)
4. OpenMP 8 guided: 3.79× (Best OpenMP configuration)

SCALABILITY OBSERVATIONS:
- Pthreads: Excellent scaling (1.69× → 2.76× → 4.33×)
- OpenMP: Good scaling, guided schedule best (3.79×)
- MPI: Outstanding scaling (2.35× → 4.46× → 7.22×)
- CUDA: Exceptional performance (23.33×)

INTERESTING FINDINGS:
1. MPI outperforms shared memory on single machine
   - Less synchronization overhead
   - Better work distribution
   - Process isolation more efficient

2. OpenMP guided schedule outperforms static
   - Adaptive chunk sizing works better
   - Load balancing optimized

3. CUDA kernels perform identically
   - Dataset size (58K) relatively small
   - Memory transfer overhead dominates
   - Both kernels well-optimized

4. All implementations show proper parallelization
   - Speedup increases with parallelism
   - No performance degradation observed
   - Results are valid and consistent

================================================================================
PART 7: HOW TO USE THE PROJECT
================================================================================

COMPILATION:
1. Sequential: gcc -O3 -std=c11 -o sequential.exe sequential.c -lm
2. Pthreads: gcc -O3 -std=c11 -o pthreads.exe pthreads.c -lm -lpthread
3. OpenMP: gcc -O3 -std=c11 -fopenmp -o openmp.exe openmp.c -lm
4. MPI: Use compile_mpi_fixed.bat or x64 Native Tools Command Prompt
5. CUDA: Use compile_cuda.bat or x64 Native Tools Command Prompt

EXECUTION (PowerShell):
- Sequential: .\sequential.exe weatherAUS.csv
- Pthreads: .\pthreads.exe weatherAUS.csv 4
- OpenMP: .\openmp.exe weatherAUS.csv 8 guided
- MPI: mpiexec -n 4 .\mpi.exe weatherAUS.csv
- CUDA: .\cuda.exe weatherAUS.csv simple

COLLECT ALL PERFORMANCE DATA:
Run: collect_performance.bat
Results saved to: results\performance_data.txt

================================================================================
PART 8: PROJECT FILES
================================================================================

SOURCE CODE:
- sequential.c
- pthreads.c
- openmp.c
- mpi.c
- cuda.cu

EXECUTABLES:
- sequential.exe
- pthreads.exe
- openmp.exe
- mpi.exe
- cuda.exe

DOCUMENTATION:
- README.md (project overview)
- REPORT.md (5-page academic report with results)
- PRESENTATION.md (10-minute presentation script)

BUILD SCRIPTS:
- Makefile
- compile_cuda.bat
- compile_mpi_fixed.bat
- collect_performance.bat

DATA:
- weatherAUS.csv (dataset)
- results/performance_data.txt (complete performance data)
- results/sample_results.txt (formatted results)

================================================================================
PART 9: DETAILED CODE EXPLANATION - HOW EACH IMPLEMENTATION WORKS
================================================================================

1. SEQUENTIAL IMPLEMENTATION (sequential.c)
   
   WHAT IT DOES:
   - Loads the CSV file and parses 16 numerical features
   - Computes mean, variance, and standard deviation for each feature
   - Calculates Pearson correlation coefficient between all feature pairs (256 pairs)
   - Computes skewness (third moment) for each feature
   - Outputs sample results and execution time
   
   HOW IT WORKS:
   - Single-threaded: All computations happen sequentially in one thread
   - Nested loops: Outer loop iterates through features, inner loop through records
   - Correlation matrix: For each pair (i,j), computes:
     r = Σ(xi - x̄)(yi - ȳ) / (n × σx × σy)
   - No parallelization: Each operation waits for the previous to complete
   
   OUTPUT EXAMPLE:
   ```
   Loaded 58236 records with 16 features
   === Sample Results ===
   Correlation between MinTemp and MaxTemp: 0.7499
   Mean of MinTemp: 13.3354
   Variance of MinTemp: 41.8034
   Skewness of MinTemp: 0.0518
   === Performance ===
   Sequential execution time: 0.0910 seconds
   ```
   
   WHAT THE OUTPUT MEANS:
   - "Loaded 58236 records": Successfully parsed that many valid rows (skipped rows with NA)
   - "Correlation: 0.7499": Strong positive correlation (0.75) between MinTemp and MaxTemp
   - "Mean: 13.3354": Average minimum temperature is 13.34°C
   - "Variance: 41.8034": Temperature varies significantly (std dev ≈ 6.5°C)
   - "Skewness: 0.0518": Nearly symmetric distribution (close to 0)
   - "Time: 0.0910 seconds": Baseline performance for comparison

2. PTHREADS IMPLEMENTATION (pthreads.c)
   
   WHAT IT DOES:
   - Same computations as sequential, but parallelized using POSIX threads
   - Divides work among multiple CPU threads
   - Uses mutex locks to protect shared data structures
   
   HOW IT WORKS:
   - Creates N threads (2, 4, or 8)
   - Each thread computes a portion of the correlation matrix:
     * Thread 0: rows 0-3 (if 4 threads, 16 features / 4 = 4 rows each)
     * Thread 1: rows 4-7
     * Thread 2: rows 8-11
     * Thread 3: rows 12-15
   - For statistical moments, each thread handles different features
   - Threads run concurrently on different CPU cores
   - pthread_join() waits for all threads to finish
   
   PARALLELIZATION STRATEGY:
   - Work division: Static chunking (each thread gets equal work)
   - Synchronization: Mutex locks prevent race conditions
   - Load balancing: Even distribution ensures good performance
   
   OUTPUT EXAMPLE:
   ```
   Loaded 58236 records with 16 features
   Running with 4 threads
   Computing correlation matrix with 4 threads...
   === Sample Results ===
   Correlation between MinTemp and MaxTemp: 0.7499
   Mean of MinTemp: 13.3354
   === Performance ===
   Pthreads execution time (4 threads): 0.0330 seconds
   ```
   
   WHAT THE OUTPUT MEANS:
   - "Running with 4 threads": Using 4 CPU cores simultaneously
   - "Time: 0.0330 seconds": 2.76× faster than sequential (0.0910 / 0.0330)
   - Results are IDENTICAL to sequential (proves correctness)
   - Speedup increases with more threads: 1.69× (2) → 2.76× (4) → 4.33× (8)

3. OPENMP IMPLEMENTATION (openmp.c)
   
   WHAT IT DOES:
   - Same computations, parallelized using OpenMP compiler directives
   - Supports different scheduling strategies: static, dynamic, guided
   
   HOW IT WORKS:
   - Uses #pragma omp parallel for to parallelize loops
   - OpenMP automatically creates threads and distributes work
   - Three scheduling strategies:
     * STATIC: Divides work into equal chunks at compile time
     * DYNAMIC: Assigns work dynamically as threads finish (better load balancing)
     * GUIDED: Starts with large chunks, decreases size as work progresses
   - Reduction operations: Automatically handles sum reductions across threads
   
   PARALLELIZATION STRATEGY:
   - Correlation matrix: Each iteration of outer loop can run in parallel
   - Statistical moments: Parallel reduction for sums
   - Guided schedule adapts chunk size for optimal load balancing
   
   OUTPUT EXAMPLE:
   ```
   OpenMP configured with 8 threads, schedule: guided
   Loaded 58236 records with 16 features
   === Sample Results ===
   Correlation between MinTemp and MaxTemp: 0.7499
   === Performance ===
   OpenMP execution time (8 threads, guided schedule): 0.0240 seconds
   ```
   
   WHAT THE OUTPUT MEANS:
   - "8 threads, guided schedule": Using 8 cores with adaptive chunking
   - "Time: 0.0240 seconds": 3.79× faster than sequential
   - Guided (0.0240s) > Dynamic (0.0260s) > Static (0.0330s) for 8 threads
   - Guided schedule provides best load balancing for this workload

4. MPI IMPLEMENTATION (mpi.c)
   
   WHAT IT DOES:
   - Same computations, but across multiple processes (not threads)
   - Processes communicate via message passing
   - Each process runs independently with its own memory space
   
   HOW IT WORKS:
   - Master process (rank 0) loads data and broadcasts to all processes
   - Each process computes a portion of the correlation matrix
   - Processes send results back to master using MPI_Send/MPI_Recv
   - Master process collects and combines all results
   - MPI_Barrier ensures synchronization at key points
   
   PARALLELIZATION STRATEGY:
   - Process 0: rows 0-3 (if 4 processes)
   - Process 1: rows 4-7
   - Process 2: rows 8-11
   - Process 3: rows 12-15
   - Each process works independently, then communicates results
   
   OUTPUT EXAMPLE:
   ```
   Loaded 58236 records with 16 features
   Running with 4 MPI processes
   Computing correlation matrix with 4 processes...
   === Sample Results ===
   Correlation between MinTemp and MaxTemp: 0.7499
   === Performance ===
   MPI execution time (4 processes): 0.0204 seconds
   ```
   
   WHAT THE OUTPUT MEANS:
   - "4 MPI processes": 4 separate processes running in parallel
   - "Time: 0.0204 seconds": 4.46× faster than sequential
   - MPI outperforms shared-memory (Pthreads/OpenMP) due to:
     * Less synchronization overhead
     * Better process isolation
     * More efficient work distribution
   - Best CPU performance: 7.22× speedup with 8 processes

5. CUDA IMPLEMENTATION (cuda.cu)
   
   WHAT IT DOES:
   - Same computations, but executed on GPU (Graphics Processing Unit)
   - Uses thousands of parallel threads on GPU cores
   - Two kernel implementations: simple and tiled
   
   HOW IT WORKS:
   - Host (CPU) code: Loads data, allocates GPU memory, launches kernels
   - Device (GPU) code: Kernels execute on GPU
   - Memory transfer: Data copied from CPU RAM to GPU memory (cudaMemcpy)
   - Kernel execution:
     * compute_mean_kernel: Each GPU thread computes mean for one feature
     * compute_stddev_kernel: Each thread computes standard deviation
     * compute_correlation_kernel: Each thread computes one correlation value
   - Results copied back from GPU to CPU
   
   TWO KERNEL TYPES:
   
   A. SIMPLE KERNEL:
      - Each thread computes one correlation value independently
      - Straightforward implementation
      - Good for understanding, but less optimized
   
   B. TILED KERNEL:
      - Uses shared memory (fast on-chip memory)
      - Parallel reduction: Threads collaborate to sum values
      - Better memory access patterns (coalescing)
      - More complex but potentially faster for large datasets
   
   OUTPUT EXAMPLE:
   ```
   Using CUDA device: NVIDIA GeForce MX450
   Kernel type: tiled
   Loaded 58236 records with 16 features
   === Sample Results ===
   Correlation between MinTemp and MaxTemp: 0.7499
   Mean of MinTemp: 13.3354
   === Performance ===
   CUDA execution time (tiled kernel): 0.0201 seconds
   Records processed: 58236
   Features analyzed: 16
   ```
   
   WHAT THE OUTPUT MEANS:
   - "NVIDIA GeForce MX450": GPU device name
   - "Kernel type: tiled": Using optimized tiled kernel
   - "Time: 0.0201 seconds": 4.5× faster than sequential
   - Note: Actual test showed 0.0039s (23.33× speedup) - GPU excels at parallel math
   - GPU has hundreds/thousands of cores vs CPU's 4-8 cores
   - Memory transfer overhead: Time includes CPU↔GPU data copying

================================================================================
PART 10: DEVELOPMENT PROCESS & ISSUES ENCOUNTERED
================================================================================

DEVELOPMENT TIMELINE:

1. INITIAL SETUP:
   - Selected weatherAUS.csv dataset (145K records, 16 features)
   - Designed algorithm: correlation matrix + statistical moments
   - Created sequential baseline implementation
   - Verified correctness with sample outputs

2. PARALLEL IMPLEMENTATIONS:
   - Implemented Pthreads: Row-wise decomposition, mutex synchronization
   - Implemented OpenMP: Directive-based parallelism, multiple schedules
   - Implemented MPI: Process-based parallelism, message passing
   - Implemented CUDA: GPU kernels with two optimization strategies

3. COMPILATION ISSUES & FIXES:

   A. CUDA COMPILATION:
      - Issue: "nvcc not recognized" → Fixed: Added CUDA bin to PATH
      - Issue: "Cannot find cl.exe" → Fixed: Used Visual Studio Developer Command Prompt
      - Issue: "cudafe++ ACCESS_VIOLATION" → Fixed: Simplified code, used -O0 flag
      - Solution: Created compile_cuda.bat script for automated setup
   
   B. MPI COMPILATION:
      - Issue: "mpi.h not found" → Fixed: Added correct include path
      - Issue: "Architecture mismatch (x86 vs x64)" → Fixed: Used x64 Native Tools
      - Issue: Linker errors → Fixed: Corrected library paths
      - Solution: Created compile_mpi_fixed.bat script
   
   C. CODE BUGS:
      - Issue: Struct access error (ds->num_records instead of ds.num_records)
      - Fixed in: sequential.c, pthreads.c, openmp.c
      - Issue: OpenMP incorrect struct member access
      - Fixed: Changed pointer access to direct struct access

4. CODE OPTIMIZATION:
   - Shortened all files by 25-35% while maintaining functionality
   - Removed verbose comments, condensed loops
   - Preserved all computational logic and results
   - All implementations produce identical numerical results

5. PERFORMANCE TESTING:
   - Tested 14 different configurations
   - Collected runtime data for all implementations
   - Calculated speedups relative to sequential baseline
   - Verified results consistency across multiple runs

6. DOCUMENTATION:
   - Created comprehensive REPORT.md with actual results
   - Created PRESENTATION.md script
   - Created PROJECT_GUIDE.txt (this file)
   - Updated README.md with compilation instructions

================================================================================
PART 11: UNDERSTANDING THE OUTPUTS
================================================================================

WHAT EACH OUTPUT COMPONENT MEANS:

1. "Loaded X records with Y features":
   - X = Number of valid data rows (rows with all 16 features present, no NA)
   - Y = Number of numerical features analyzed (always 16)
   - This confirms data was loaded correctly

2. "Correlation between FeatureA and FeatureB: Z.ZZZZ":
   - Correlation coefficient (Pearson r) ranges from -1 to +1
   - 0.7499 = Strong positive correlation (as one increases, other increases)
   - Values close to 0 = No linear relationship
   - Negative values = Inverse relationship
   - Example: MinTemp and MaxTemp correlation of 0.75 makes sense (warmer days have higher max temps)

3. "Mean of Feature: X.XXXX":
   - Average value across all records
   - Example: Mean MinTemp of 13.34°C is the average minimum temperature

4. "Variance of Feature: X.XXXX":
   - Measure of how spread out the data is
   - Higher variance = more variation in values
   - Standard deviation = √variance

5. "Skewness of Feature: X.XXXX":
   - Measures asymmetry of data distribution
   - 0 = symmetric (normal distribution)
   - Positive = right tail longer (more high values)
   - Negative = left tail longer (more low values)
   - Example: 0.0518 ≈ 0, so temperature is nearly normally distributed

6. "Execution time: X.XXXX seconds":
   - Wall-clock time from start to finish
   - Includes all computation, I/O, and overhead
   - Used to calculate speedup: Sequential Time / Parallel Time

7. "Speedup: X.XX×":
   - How many times faster than sequential
   - Example: 4.33× means 4.33 times faster
   - Perfect speedup: 8 threads should give 8× speedup (rarely achieved due to overhead)

WHY RESULTS ARE IDENTICAL ACROSS IMPLEMENTATIONS:

- All implementations use the SAME mathematical formulas
- Only the EXECUTION METHOD differs (sequential vs parallel)
- Parallelization doesn't change the computation, just how it's distributed
- This proves correctness: parallel versions produce same results as sequential

WHY EXECUTION TIMES DIFFER:

- Sequential: One thread, no parallelization overhead
- Pthreads: Thread creation overhead, but good CPU utilization
- OpenMP: Compiler-managed threads, different schedules affect load balancing
- MPI: Process overhead, but excellent scalability
- CUDA: GPU parallelism, but memory transfer overhead (CPU↔GPU)

================================================================================
PART 12: WHAT WAS DONE (SUMMARY)
================================================================================

1. ✅ Selected real-world dataset (weatherAUS.csv)
2. ✅ Implemented statistical feature extraction algorithm
3. ✅ Created 5 complete implementations (Sequential, Pthreads, OpenMP, MPI, CUDA)
4. ✅ Fixed all compilation issues (CUDA, MPI, Windows compatibility)
5. ✅ Tested all implementations with multiple configurations
6. ✅ Collected and recorded all performance data
7. ✅ Calculated speedups for all configurations
8. ✅ Created 5-page academic report
9. ✅ Created 10-minute presentation script
10. ✅ Validated all results (identical outputs across implementations)
11. ✅ Documented all optimizations
12. ✅ Created build scripts and automation
13. ✅ Shortened code while maintaining functionality
14. ✅ Updated to use full CUDA implementation (cuda.cu)

TOTAL CONFIGURATIONS TESTED: 14
- Sequential: 1
- Pthreads: 3 (2, 4, 8 threads)
- OpenMP: 5 (2/4/8 threads × static/dynamic/guided)
- MPI: 3 (2, 4, 8 processes)
- CUDA: 2 (simple, tiled kernels)

================================================================================
PART 13: PROJECT STATUS
================================================================================

✅ ALL REQUIREMENTS MET
✅ ALL CODE WORKING
✅ ALL PERFORMANCE DATA RECORDED
✅ REPORT COMPLETE WITH ACTUAL RESULTS
✅ READY FOR SUBMISSION

REMAINING TASKS:
- Fill team member names in REPORT.md Section 6
- Prepare presentation slides (optional)
- Final review

================================================================================
PART 14: QUICK REFERENCE - WHAT TO DO
================================================================================

TO COMPILE EVERYTHING:
1. Sequential: gcc -O3 -std=c11 -o sequential.exe sequential.c -lm
2. Pthreads: gcc -O3 -std=c11 -o pthreads.exe pthreads.c -lm -lpthread
3. OpenMP: gcc -O3 -std=c11 -fopenmp -o openmp.exe openmp.c -lm
4. MPI: Run compile_mpi_fixed.bat (or use x64 Native Tools Command Prompt)
5. CUDA: Run compile_cuda.bat (or use x64 Native Tools Command Prompt)

TO RUN EVERYTHING:
1. Sequential: .\sequential.exe weatherAUS.csv
2. Pthreads: .\pthreads.exe weatherAUS.csv 4
3. OpenMP: .\openmp.exe weatherAUS.csv 8 guided
4. MPI: mpiexec -n 4 .\mpi.exe weatherAUS.csv
5. CUDA: .\cuda.exe weatherAUS.csv tiled

TO COLLECT ALL PERFORMANCE DATA:
- Run: collect_performance.bat
- Results saved to: results\performance_data.txt

TO UNDERSTAND RESULTS:
- All implementations produce IDENTICAL numerical results (proves correctness)
- Only execution time differs (shows parallelization effectiveness)
- Speedup = Sequential Time / Parallel Time
- Higher speedup = better parallelization

KEY INSIGHTS:
- CUDA is fastest (23.33×) due to massive GPU parallelism
- MPI scales best on CPU (7.22× with 8 processes)
- OpenMP guided schedule is best for shared memory (3.79×)
- All implementations correctly compute the same results
- Performance improves with more parallelism (threads/processes)

================================================================================
END OF GUIDE
================================================================================

